{"cells":[{"cell_type":"markdown","metadata":{"id":"MNBgGYg_lpVN"},"source":["# Assignment Module 2: Product Classification\n","\n","The goal of this assignment is to implement a neural network that classifies smartphone pictures of products found in grocery stores. The assignment will be divided into two parts: first, you will be asked to implement from scratch your own neural network for image classification; then, you will fine-tune a pretrained network provided by PyTorch.\n"]},{"cell_type":"markdown","metadata":{"id":"dVTQUJ4uYH1w"},"source":["## Preliminaries: the dataset\n","\n","The dataset you will be using contains natural images of products taken with a smartphone camera in different grocery stores:\n","\n","<p align=\"center\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Granny-Smith.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Pink-Lady.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Lemon.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Banana.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Vine-Tomato.jpg\" width=\"150\">\n","</p>\n","<p align=\"center\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Yellow-Onion.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Green-Bell-Pepper.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Arla-Standard-Milk.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Oatly-Natural-Oatghurt.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Alpro-Fresh-Soy-Milk.jpg\" width=\"150\">\n","</p>\n","\n","The products belong to the following 43 classes:\n","```\n","0.  Apple\n","1.  Avocado\n","2.  Banana\n","3.  Kiwi\n","4.  Lemon\n","5.  Lime\n","6.  Mango\n","7.  Melon\n","8.  Nectarine\n","9.  Orange\n","10. Papaya\n","11. Passion-Fruit\n","12. Peach\n","13. Pear\n","14. Pineapple\n","15. Plum\n","16. Pomegranate\n","17. Red-Grapefruit\n","18. Satsumas\n","19. Juice\n","20. Milk\n","21. Oatghurt\n","22. Oat-Milk\n","23. Sour-Cream\n","24. Sour-Milk\n","25. Soyghurt\n","26. Soy-Milk\n","27. Yoghurt\n","28. Asparagus\n","29. Aubergine\n","30. Cabbage\n","31. Carrots\n","32. Cucumber\n","33. Garlic\n","34. Ginger\n","35. Leek\n","36. Mushroom\n","37. Onion\n","38. Pepper\n","39. Potato\n","40. Red-Beet\n","41. Tomato\n","42. Zucchini\n","```\n","\n","The dataset is split into training (`train`), validation (`val`), and test (`test`) set."]},{"cell_type":"markdown","metadata":{"id":"1pdrmJRnJPd8"},"source":["The following code cells download the dataset and define a `torch.utils.data.Dataset` class to access it. This `Dataset` class will be the starting point of your assignment: use it in your own code and build everything else around it."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"POMX_3x-_bZI"},"outputs":[],"source":["try:\n","    import google.colab\n","    !git clone https://github.com/marcusklasson/GroceryStoreDataset.git\n","except: pass"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"hiF8xGEYlsu8"},"outputs":[],"source":["from pathlib import Path\n","from PIL import Image\n","from torch import Tensor\n","from torch.utils.data import Dataset\n","from typing import List, Tuple"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"jROSO2qVDxdD"},"outputs":[],"source":["class GroceryStoreDataset(Dataset):\n","\n","    def __init__(self, split: str, transform=None) -> None:\n","        super().__init__()\n","\n","        self.root = Path(\"GroceryStoreDataset/dataset\")\n","        self.split = split\n","        self.paths, self.labels = self.read_file()\n","\n","        self.transform = transform\n","\n","    def __len__(self) -> int:\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx) -> Tuple[Tensor, int]:\n","        img = Image.open(self.root / self.paths[idx])\n","        label = self.labels[idx]\n","\n","        if self.transform:\n","            img = self.transform(img)\n","\n","        return img, label\n","\n","    def read_file(self) -> Tuple[List[str], List[int]]:\n","        paths = []\n","        labels = []\n","\n","        with open(self.root / f\"{self.split}.txt\") as f:\n","            for line in f:\n","                # path, fine-grained class, coarse-grained class\n","                path, _, label = line.replace(\"\\n\", \"\").split(\", \")\n","                paths.append(path), labels.append(int(label))\n","\n","        return paths, labels\n","\n","    def get_num_classes(self) -> int:\n","        return max(self.labels) + 1"]},{"cell_type":"markdown","metadata":{"id":"yBch3dpwNSsW"},"source":["## Part 1: design your own network\n","\n","Your goal is to implement a convolutional neural network for image classification and train it on `GroceryStoreDataset`. You should consider yourselves satisfied once you obtain a classification accuracy on the **validation** split of **around 60%**. You are free to achieve that however you want, except for a few rules you must follow:\n","\n","- You **cannot** simply instantiate an off-the-self PyTorch network. Instead, you must construct your network as a composition of existing PyTorch layers. In more concrete terms, you can use e.g. `torch.nn.Linear`, but you **cannot** use e.g. `torchvision.models.alexnet`.\n","\n","- Justify every *design choice* you make. Design choices include network architecture, training hyperparameters, and, possibly, dataset preprocessing steps. You can either (i) start from the simplest convolutional network you can think of and add complexity one step at a time, while showing how each step gets you closer to the target ~60%, or (ii) start from a model that is already able to achieve the desired accuracy and show how, by removing some of its components, its performance drops (i.e. an *ablation study*). You can *show* your results/improvements however you want: training plots, console-printed values or tables, or whatever else your heart desires: the clearer, the better.\n","\n","Don't be too concerned with your network performance: the ~60% is just to give you an idea of when to stop. Keep in mind that a thoroughly justified model with lower accuracy will be rewarded **more** points than a poorly experimentally validated model with higher accuracy."]},{"cell_type":"markdown","metadata":{},"source":["### Imports"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# ! python3.11 -m pip install --upgrade pip\n","# ! pip install -q wandb\n","# ! pip install -q torchmetrics\n","# ! pip install torchsummary"]},{"cell_type":"markdown","metadata":{},"source":["### Weights and Biases for debug"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import wandb\n","\n","WANDB_USER = \"lollopelle-2-universit-di-bologna\"  # insert your wandb username here\n","WANDB_PROJECT = \"IPCV-assignment-2\""]},{"cell_type":"markdown","metadata":{},"source":["### Imports"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# TODO: clean\n","\n","from tqdm import tqdm\n","import numpy as np\n","import torch\n","import random\n","import torch.utils\n","from torchvision import transforms as T\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","from torch.optim import Adam\n","import torch.nn.functional as F\n","\n","import matplotlib.pyplot as plt\n","import math\n","import csv\n","\n","import copy\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","import requests\n","import shutil\n","import tarfile\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from collections import defaultdict\n","from pathlib import Path\n","from PIL import Image\n","from tqdm.notebook import tqdm\n","from typing import Any, Callable, Dict, List, Optional\n","\n","from torch.utils.data import DataLoader\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import OneCycleLR\n","from torchmetrics.classification.accuracy import Accuracy\n","from torchsummary import summary\n","from torchvision import transforms as T\n","from torchvision.datasets import ImageFolder\n","from torchvision.models import mobilenet_v2, MobileNet_V2_Weights, resnet18, ResNet18_Weights"]},{"cell_type":"markdown","metadata":{},"source":["#### Functions"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def fix_random(seed: int) -> None:\n","    \"\"\"Fix all the possible sources of randomness.\n","\n","    Args:\n","        seed: the seed to use.\n","    \"\"\"\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","    \n","def extract_classes(csv_file_path: str) -> dict:\n","    \"\"\"\n","    Extract unique pairs of IDs and labels from a CSV file.\n","\n","    This function reads a CSV file, extracts the third and fourth columns,\n","    and creates a dictionary with unique pairs of IDs (from the fourth column)\n","    and labels (from the third column).\n","\n","    Parameters:\n","    csv_file_path (str): The path to the CSV file.\n","\n","    Returns:\n","    dict: A dictionary with IDs as keys and labels as values.\n","    \"\"\"\n","    \n","    # Dictionary to store the unique pairs\n","    classes = {}\n","\n","    # Read the CSV file\n","    with open(csv_file_path, mode='r', newline='', encoding='utf-8') as file:\n","        csv_reader = csv.reader(file)\n","        \n","        # Skip the CSV header\n","        next(csv_reader)\n","        \n","        for row in csv_reader:\n","            label = row[2]       # Third column\n","            id = int(row[3])     # Fourth column\n","            \n","            # Add the pair to the dictionary if it doesn't already exist\n","            if id not in classes:\n","                classes[id] = label\n","    \n","    return classes\n","\n","def show_grid(dataset: GroceryStoreDataset, classes: dict) -> None:\n","    \"\"\"Shows a grid with random images taken from the dataset.\n","\n","    Args:\n","        dataset: the dataset containing the images.\n","        process: a function to apply on the images before showing them.\n","    \"\"\"\n","    fig = plt.figure(figsize=(15, 5))\n","    indices_random = np.random.randint(10, size=10, high=len(classes.keys()))\n","\n","    for count, idx in enumerate(indices_random):\n","        fig.add_subplot(2, 5, count + 1)\n","        item = dataset.__getitem__(idx) # (Tensor, idx)\n","        title = classes[item[1]]\n","        plt.title(title)\n","        image_processed = item[0]\n","        plt.imshow(T.ToPILImage()(image_processed))\n","        plt.axis(\"off\")\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Configuration"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Please set GPU via Edit -> Notebook Settings\n"]}],"source":["fix_random(seed=42)\n","\n","device = \"cpu\"\n","if torch.cuda.is_available():\n","  print(\"All good, a GPU is available\")\n","  device = torch.device(\"cuda:0\")\n","else:\n","  print(\"Please set GPU via Edit -> Notebook Settings\")\n","\n","cfg = {\n","    \"resize_size\": 256,\n","    \"crop_size\": 224,\n","\n","    \"batch_size\": 4,\n","    \"num_epochs\": 20,\n","\n","    \"lr\": 1e-3,\n","    \"wd\": 1e-4,\n","    \"step_size\": 5\n","}"]},{"cell_type":"markdown","metadata":{},"source":["### Data"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# In  order to convert integer classes into their literal\n","classes = extract_classes(csv_file_path = 'GroceryStoreDataset/dataset/classes.csv')\n","\n","# Preprocessing\n","mean_image_net = [0.485, 0.456, 0.406]                              # FIXME\n","std_image_net = [0.229, 0.224, 0.225]                               # FIXME\n","data_transforms = {\n","    \"train\": T.Compose([\n","                        T.RandomResizedCrop(cfg[\"crop_size\"]),      # FIXME\n","                        T.RandomHorizontalFlip(),                   # FIXME\n","                        T.ToTensor(),                               # FIXME\n","                        T.Normalize(mean_image_net, std_image_net)  # FIXME\n","                    ]),\n","\n","    \"val\": T.Compose([\n","                        T.Resize(cfg[\"resize_size\"]),               # FIXME\n","                        T.CenterCrop(cfg[\"crop_size\"]),             # FIXME\n","                        T.ToTensor(),                               # FIXME\n","                        T.Normalize(mean_image_net, std_image_net)  # FIXME\n","                    ]),\n","   \n","    \"test\": T.Compose([T.ToTensor()]) # DEBUG\n","}\n","\n","# Datasets\n","data_train = GroceryStoreDataset(split=\"train\", transform=data_transforms[\"train\"])\n","data_val = GroceryStoreDataset(split=\"val\", transform=data_transforms[\"val\"])\n","data_test = GroceryStoreDataset(split=\"test\", transform=data_transforms[\"test\"])\n","\n","# DEBUG \n","# show_grid(dataset=data_train, classes=classes)\n","# show_grid(dataset=data_test, classes=classes)\n","# show_grid(dataset=data_val, classes=classes)"]},{"cell_type":"markdown","metadata":{},"source":["### Model"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["class SimpleClassifier(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, n_classes, n_hidden_layers=0):\n","        super().__init__()\n","\n","        # Initialize the modules we need to build the network\n","        self.first = nn.Linear(input_dim, hidden_dim)\n","        self.activation = nn.ReLU()\n","        self.last = nn.Linear(hidden_dim, n_classes)\n","\n","        self.hidden_layers = nn.ModuleList([\n","            nn.Linear(hidden_dim, hidden_dim) for i in range(n_hidden_layers)\n","        ])\n","\n","    def forward(self, x):\n","        # Perform the calculation of the model to determine the prediction\n","        x = self.first(x)\n","        x = self.activation(x)\n","        for layer in self.hidden_layers:\n","            x = layer(x)\n","            x = self.activation(x)\n","        x = self.last(x)\n","\n","        return x\n","    \n","    def layers(self):\n","        for name, params in self.named_parameters():\n","            print(f\"{name}: {params.shape}\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["first.weight: torch.Size([128, 224])\n","first.bias: torch.Size([128])\n","last.weight: torch.Size([43, 128])\n","last.bias: torch.Size([43])\n","hidden_layers.0.weight: torch.Size([128, 128])\n","hidden_layers.0.bias: torch.Size([128])\n"]}],"source":["model = SimpleClassifier(\n","    input_dim=cfg[\"crop_size\"],\n","    hidden_dim=128,\n","    n_classes=len(classes),\n","    n_hidden_layers=1\n",")\n","    \n","model.layers()\n","    \n","# Verifies if the model is already on the device\n","if next(model.parameters()).device != device:\n","    model.to(device)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Linear-1          [-1, 3, 224, 128]          28,800\n","              ReLU-2          [-1, 3, 224, 128]               0\n","            Linear-3          [-1, 3, 224, 128]          16,512\n","              ReLU-4          [-1, 3, 224, 128]               0\n","            Linear-5           [-1, 3, 224, 43]           5,547\n","================================================================\n","Total params: 50,859\n","Trainable params: 50,859\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 2.85\n","Params size (MB): 0.19\n","Estimated Total Size (MB): 3.61\n","----------------------------------------------------------------\n"]}],"source":["summary(\n","    model,\n","    input_size=(3, cfg[\"crop_size\"], cfg[\"crop_size\"])\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Trainer"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# For automating batching\n","\n","loader_train = DataLoader(\n","    data_train,\n","    batch_size=cfg[\"batch_size\"],\n","    shuffle=True,\n","    pin_memory=True\n",")\n","loader_val = DataLoader(\n","    data_val,\n","    batch_size=cfg[\"batch_size\"],\n","    shuffle=False\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["class Trainer:\n","    def __init__(self,\n","            model: nn.Module,\n","            train_loader: DataLoader,\n","            val_loader: DataLoader,\n","            device: torch.device,\n","            num_classes: int\n","        ) -> None:\n","        self.train_loader = train_loader\n","        self.val_loader = val_loader\n","        self.device = device\n","        self.num_classes = num_classes\n","        self.num_epochs = cfg[\"num_epochs\"]\n","\n","        self.model = model.to(device)\n","        self.optimizer = AdamW(self.model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"wd\"])\n","        num_steps = self.num_epochs * len(train_loader)\n","        self.scheduler = OneCycleLR(self.optimizer, cfg[\"lr\"], total_steps=num_steps)\n","\n","        self.step = 0\n","        self.best_acc = 0.0\n","\n","        wandb.init(name=cfg[\"run_name\"], entity=WANDB_USER, project=WANDB_PROJECT, config=cfg)\n","        self.ckpt_path = Path(\"ckpts\")\n","        self.ckpt_path.mkdir(exist_ok=True)\n","\n","    def logfn(self, values: Dict[str, Any]) -> None:\n","        wandb.log(values, step=self.step, commit=False)\n","\n","    def train(self) -> None:\n","        for _ in tqdm(range(self.num_epochs), desc=\"Epoch\"):\n","            self.model.train()\n","\n","            for imgs, labels in self.train_loader:\n","                imgs = imgs.to(self.device)\n","                labels = labels.to(self.device)\n","\n","                pred = self.model(imgs)\n","                loss = F.cross_entropy(pred, labels)\n","\n","                self.optimizer.zero_grad()\n","                loss.backward()\n","                self.optimizer.step()\n","                self.scheduler.step()\n","\n","                if self.step % 10 == 0:\n","                    self.logfn({\"train/loss\": loss.item()})\n","                    self.logfn({\"train/lr\": self.scheduler.get_last_lr()[0]})\n","\n","                self.step += 1\n","\n","            self.eval(\"train\")\n","            self.eval(\"val\")\n","\n","        wandb.finish()\n","\n","    @torch.no_grad()\n","    def eval(self, split: str) -> None:\n","        self.model.eval()\n","\n","        loader = self.train_loader if split == \"train\" else self.val_loader\n","        acc = Accuracy(\"multiclass\", num_classes=self.num_classes).to(self.device)\n","\n","        losses = []\n","        for imgs, labels in loader:\n","            imgs = imgs.to(self.device)\n","            labels = labels.to(self.device)\n","\n","            pred = self.model(imgs)\n","            loss = F.cross_entropy(pred, labels)\n","            losses.append(loss.item())\n","\n","            pred_softmax = F.softmax(pred, dim=-1)\n","            acc(pred_softmax, labels)\n","\n","        loss = sum(losses) / len(losses)\n","        accuracy = acc.compute()\n","\n","        self.logfn({f\"{split}/loss\": loss})\n","        self.logfn({f\"{split}/acc\": accuracy})\n","\n","        if accuracy > self.best_acc and split == \"val\":\n","            self.best_acc = accuracy\n","            torch.save(self.model.state_dict(), self.ckpt_path / f\"{wandb.run.name}.pt\")\n","            self.best_model = copy.deepcopy(self.model)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/pelle/.netrc\n"]},{"data":{"text/html":["Tracking run with wandb version 0.17.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/Users/pelle/Development/GitHub/IPCV-assignments/part_two/pelle/wandb/run-20240724_160805-h2e01wdq</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/lollopelle-2-universit-di-bologna/IPCV-assignment-2/runs/h2e01wdq' target=\"_blank\">prova</a></strong> to <a href='https://wandb.ai/lollopelle-2-universit-di-bologna/IPCV-assignment-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/lollopelle-2-universit-di-bologna/IPCV-assignment-2' target=\"_blank\">https://wandb.ai/lollopelle-2-universit-di-bologna/IPCV-assignment-2</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/lollopelle-2-universit-di-bologna/IPCV-assignment-2/runs/h2e01wdq' target=\"_blank\">https://wandb.ai/lollopelle-2-universit-di-bologna/IPCV-assignment-2/runs/h2e01wdq</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e656c3b9c9224445be4cbaa84c157387","version_major":2,"version_minor":0},"text/plain":["Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"RuntimeError","evalue":"only batches of spatial targets supported (3D tensors) but got targets of dimension: 1","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprova\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      4\u001b[0m     model,\n\u001b[1;32m      5\u001b[0m     loader_train,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(classes\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[14], line 39\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     38\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(imgs)\n\u001b[0;32m---> 39\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     42\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: only batches of spatial targets supported (3D tensors) but got targets of dimension: 1"]}],"source":["cfg[\"run_name\"] = \"prova\"\n","\n","trainer = Trainer(\n","    model,\n","    loader_train,\n","    loader_val,\n","    device,\n","    num_classes=len(classes.keys())\n",")\n","trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"gkWEqSPoUIL3"},"source":["## Part 2: fine-tune an existing network\n","\n","Your goal is to fine-tune a pretrained **ResNet-18** model on `GroceryStoreDataset`. Use the implementation provided by PyTorch, do not implement it yourselves! (i.e. exactly what you **could not** do in the first part of the assignment). Specifically, you must use the PyTorch ResNet-18 model pretrained on ImageNet-1K (V1). Divide your fine-tuning into two parts:\n","\n","1. First, fine-tune the Resnet-18 with the same training hyperparameters you used for your best model in the first part of the assignment.\n","1. Then, tweak the training hyperparameters in order to increase the accuracy on the validation split of `GroceryStoreDataset`. Justify your choices by analyzing the training plots and/or citing sources that guided you in your decisions (papers, blog posts, YouTube videos, or whatever else you find enlightening). You should consider yourselves satisfied once you obtain a classification accuracy on the **validation** split **between 80 and 90%**."]}],"metadata":{"colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
